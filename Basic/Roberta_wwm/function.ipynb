{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: UTF-8\n",
    "# use model \"chinese_roberta_wwm_ext_pytorch\"\n",
    "import torch\n",
    "import time \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from pytorch_pretrained_bert import BertModel, BertTokenizer, BertConfig, BertAdam\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import *\n",
    "#CUDA_DEVICE_ORDER=\"PCI_BUS_ID\"  CUDA_VISIBLE_DEVICES=1 python function.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "124691it [00:15, 8122.99it/s]\n"
    }
   ],
   "source": [
    "path = \"data/\"\n",
    "bert_path = \"chinese_roberta_wwm_ext_pytorch/\"\n",
    "tokenizer = BertTokenizer(vocab_file=bert_path + \"vocab.txt\")  # 初始化分词器\n",
    "\n",
    "input_ids = []     # input char ids\n",
    "input_types = []   # segment ids\n",
    "input_masks = []   # attention mask\n",
    "label = []         # 标签\n",
    "pad_size = 38   # 也称为 max_len (前期统计分析，文本长度最大值为38，取32即可覆盖99%)\n",
    " \n",
    "with open(path + \"train.txt\", encoding='utf-8') as f:\n",
    "    for i, l in tqdm(enumerate(f)): \n",
    "        x1, y = l.strip().split('\\t')\n",
    "        x1 = tokenizer.tokenize(x1)\n",
    "        tokens = [\"[CLS]\"] + x1 + [\"[SEP]\"]\n",
    "        \n",
    "        # 得到input_id, seg_id, att_mask\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        types = [0] *(len(ids))\n",
    "        masks = [1] * len(ids)\n",
    "        # 短则补齐，长则切断\n",
    "        if len(ids) < pad_size:\n",
    "            types = types + [1] * (pad_size - len(ids))  # mask部分 segment置为1\n",
    "            masks = masks + [0] * (pad_size - len(ids))\n",
    "            ids = ids + [0] * (pad_size - len(ids))\n",
    "        else:\n",
    "            types = types[:pad_size]\n",
    "            masks = masks[:pad_size]\n",
    "            ids = ids[:pad_size]\n",
    "        input_ids.append(ids)\n",
    "        input_types.append(types)\n",
    "        input_masks.append(masks)\n",
    "#         print(len(ids), len(masks), len(types)) \n",
    "        assert len(ids) == len(masks) == len(types) == pad_size\n",
    "        label.append([int(y)])\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(99752, 38) (99752, 38) (99752, 38) (99752, 1)\n(24939, 38) (24939, 38) (24939, 38) (24939, 1)\n"
    }
   ],
   "source": [
    "# 随机打乱索引\n",
    "random_order = list(range(len(input_ids)))\n",
    "np.random.seed(2020)   # 固定种子\n",
    "np.random.shuffle(random_order)\n",
    "#print(random_order[:21])\n",
    "\n",
    "# 4:1 划分训练集和测试集\n",
    "input_ids_train = np.array([input_ids[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_types_train = np.array([input_types[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "input_masks_train = np.array([input_masks[i] for i in random_order[:int(len(input_ids)*0.8)]])\n",
    "y_train = np.array([label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "print(input_ids_train.shape, input_types_train.shape, input_masks_train.shape, y_train.shape)\n",
    "\n",
    "input_ids_test = np.array([input_ids[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_types_test = np.array([input_types[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "input_masks_test = np.array([input_masks[i] for i in random_order[int(len(input_ids)*0.8):]])\n",
    "y_test = np.array([label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "print(input_ids_test.shape, input_types_test.shape, input_masks_test.shape, y_test.shape)\n",
    "\n",
    "BATCH_SIZE = 24\n",
    "train_data = TensorDataset(torch.LongTensor(input_ids_train), \n",
    "                           torch.LongTensor(input_types_train), \n",
    "                           torch.LongTensor(input_masks_train), \n",
    "                           torch.LongTensor(y_train))\n",
    "train_sampler = RandomSampler(train_data)  \n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data = TensorDataset(torch.LongTensor(input_ids_test), \n",
    "                          torch.LongTensor(input_types_test), \n",
    "                         torch.LongTensor(input_masks_test),\n",
    "                          torch.LongTensor(y_test))\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda\nModel(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): BertLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (fc): Linear(in_features=768, out_features=21, bias=True)\n)\n"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_path)  # /bert_pretrain/\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True  # 每个参数都要 求梯度 feiyu 改成false\n",
    "        self.fc = nn.Linear(768, 21)   # 768 -> 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]  # 输入的句子   (ids, seq_len, mask)\n",
    "        types = x[1]\n",
    "        mask = x[2]  # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, token_type_ids=types, \n",
    "                              attention_mask=mask, \n",
    "                              output_all_encoded_layers=False) # 控制是否输出所有encoder层的结果\n",
    "        out = self.fc(pooled)   # 得到10分类\n",
    "        return out\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#DEVICE = 'cpu'\n",
    "print(DEVICE)\n",
    "model = Model().to(DEVICE)\n",
    "\n",
    "print(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())  # 模型参数名字列表\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "NUM_EPOCHS = 1\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=0.05,\n",
    "                     t_total=len(train_loader) * NUM_EPOCHS\n",
    "                    )\n",
    "\n",
    "def select_predict(test_y,pred_y,device):\n",
    "    select_y_pred=torch.Tensor(pred_y.shape[0],3).to(device)\n",
    "    for i in range(pred_y.shape[0]):\n",
    "        pos = test_y[i]//3 ## 找到对应的位置\n",
    "        select_y_pred[i] = pred_y[i][pos:pos+3]\n",
    "        #select_y_pred = torch.cat((select_y_pred,y_select),0)\n",
    "    return select_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):   # 训练模型\n",
    "    model.train()\n",
    "    best_acc = 0.0 \n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(train_loader):\n",
    "        start_time = time.time()\n",
    "        x1,x2,x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y_pred = model([x1, x2, x3])  # 得到预测结果\n",
    "        model.zero_grad()             # 梯度清零\n",
    "        loss = F.cross_entropy(y_pred, y.squeeze())  # 得到loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if(batch_idx + 1) % 100 == 0:    # 打印loss\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1), \n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader), \n",
    "                                                                           loss.item()))  # 记得为loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_predict(test_y,pred_y):\n",
    "    select_y_pred=torch.Tensor(pred_y.shape[0],3)\n",
    "    for i in range(pred_y.shape[0]):\n",
    "        pos = int(test_y[i]//3) ## 找到对应的位置\n",
    "        select_y_pred[i] = pred_y[i][pos:pos+3]\n",
    "        #select_y_pred = torch.cat((select_y_pred,y_select),0)\n",
    "    return select_y_pred\n",
    " \n",
    "def Normalization(test_y):\n",
    "    y_norm = torch.Tensor(test_y.shape[0],1)\n",
    "    for i in range(test_y.shape[0]):\n",
    "        y_norm[i] = int(test_y[i] % 3) # 转换为 int\n",
    "    return y_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):    # 测试模型, 得到测试集评估结果\n",
    "    model.eval()\n",
    "    test_loss = 0.0 \n",
    "    acc = 0 \n",
    "\n",
    "    for batch_idx, (x1,x2,x3, y) in enumerate(test_loader):\n",
    "        x1,x2,x3, y_origin = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        y = Normalization(y_origin).to(device)\n",
    "        y = y.type_as(y_origin)\n",
    "        with torch.no_grad():#\n",
    "            y_pred = model([x1,x2,x3])      \n",
    "        y_ = select_predict(y,y_pred).to(device)\n",
    "        y_= y_.type_as(y_pred)\n",
    "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max(): 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "\n",
    "      \n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "          test_loss, acc, len(test_loader.dataset),\n",
    "          100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Epoch: 1 [2400/99752 (2.38%)]\tLoss: 2.483881\nTrain Epoch: 1 [4800/99752 (4.79%)]\tLoss: 2.504553\nTrain Epoch: 1 [7200/99752 (7.19%)]\tLoss: 2.070401\nTrain Epoch: 1 [9600/99752 (9.60%)]\tLoss: 1.875722\nTrain Epoch: 1 [12000/99752 (12.00%)]\tLoss: 2.135104\nTrain Epoch: 1 [14400/99752 (14.41%)]\tLoss: 1.975475\nTrain Epoch: 1 [16800/99752 (16.82%)]\tLoss: 2.274744\nTrain Epoch: 1 [19200/99752 (19.22%)]\tLoss: 2.036564\nTrain Epoch: 1 [21600/99752 (21.63%)]\tLoss: 1.787022\nTrain Epoch: 1 [24000/99752 (24.03%)]\tLoss: 2.312529\nTrain Epoch: 1 [26400/99752 (26.44%)]\tLoss: 1.769793\nTrain Epoch: 1 [28800/99752 (28.84%)]\tLoss: 1.859949\nTrain Epoch: 1 [31200/99752 (31.25%)]\tLoss: 1.656064\nTrain Epoch: 1 [33600/99752 (33.65%)]\tLoss: 1.819979\nTrain Epoch: 1 [36000/99752 (36.06%)]\tLoss: 2.023920\nTrain Epoch: 1 [38400/99752 (38.47%)]\tLoss: 1.807276\nTrain Epoch: 1 [40800/99752 (40.87%)]\tLoss: 1.732528\nTrain Epoch: 1 [43200/99752 (43.28%)]\tLoss: 1.933527\nTrain Epoch: 1 [45600/99752 (45.68%)]\tLoss: 1.951922\nTrain Epoch: 1 [48000/99752 (48.09%)]\tLoss: 1.616982\nTrain Epoch: 1 [50400/99752 (50.49%)]\tLoss: 1.860437\nTrain Epoch: 1 [52800/99752 (52.90%)]\tLoss: 1.912686\nTrain Epoch: 1 [55200/99752 (55.30%)]\tLoss: 2.011485\nTrain Epoch: 1 [57600/99752 (57.71%)]\tLoss: 1.787973\nTrain Epoch: 1 [60000/99752 (60.12%)]\tLoss: 2.017078\nTrain Epoch: 1 [62400/99752 (62.52%)]\tLoss: 1.743650\nTrain Epoch: 1 [64800/99752 (64.93%)]\tLoss: 1.892653\nTrain Epoch: 1 [67200/99752 (67.33%)]\tLoss: 2.003265\nTrain Epoch: 1 [69600/99752 (69.74%)]\tLoss: 2.109303\nTrain Epoch: 1 [72000/99752 (72.14%)]\tLoss: 1.893604\nTrain Epoch: 1 [74400/99752 (74.55%)]\tLoss: 1.676814\nTrain Epoch: 1 [76800/99752 (76.95%)]\tLoss: 1.842110\nTrain Epoch: 1 [79200/99752 (79.36%)]\tLoss: 2.265782\nTrain Epoch: 1 [81600/99752 (81.77%)]\tLoss: 1.721052\nTrain Epoch: 1 [84000/99752 (84.17%)]\tLoss: 1.557847\nTrain Epoch: 1 [86400/99752 (86.58%)]\tLoss: 2.139854\nTrain Epoch: 1 [88800/99752 (88.98%)]\tLoss: 1.516899\nTrain Epoch: 1 [91200/99752 (91.39%)]\tLoss: 1.906967\nTrain Epoch: 1 [93600/99752 (93.79%)]\tLoss: 2.133822\nTrain Epoch: 1 [96000/99752 (96.20%)]\tLoss: 1.713561\nTrain Epoch: 1 [98400/99752 (98.60%)]\tLoss: 1.611821\n\nTest set: Average loss: 0.4636, Accuracy: 20623/24939 (82.69%)\nacc is: 0.8269, best acc is 0.8269\n\n"
    }
   ],
   "source": [
    "best_acc = 0.0 \n",
    "PATH = 'model_on_roberta_wwm_oneepoch.pth'  # 定义模型保存路径  \n",
    "for epoch in range(1, NUM_EPOCHS+1):  # 3个epoch\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc = test(model, DEVICE, test_loader)\n",
    "    if best_acc < acc: \n",
    "        best_acc = acc \n",
    "        torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "    print(\"acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc, best_acc))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nTest set: Average loss: 0.4636, Accuracy: 20623/24939 (82.69%)\n"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(\"model_on_roberta_wwm_oneepoch.pth\"))\n",
    "acc = test(model, DEVICE, test_loader)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}